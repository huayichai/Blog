# 什么是Sketch？

最近读论文时经常能看到Sketch这个关键词，但是翻译软件给的解释是“素描、草图”，搞得我一头雾水。近期看了一篇系统介绍Sketch的文章《Data Sketing》，这里记录一下。

原文：https://dl.acm.org/doi/10.1145/3080008

PDF：https://dl.acm.org/doi/pdf/10.1145/3080008



**简单解释一下什么是Sketch：**当面临海量数据时，完全存储这些数据几乎是不可能的，或者是没有必要的，因此，人们提出了一些概率数据结构，期望能用一个较少的空间来近似表示完整的数据集，在能够容忍误报的情况下，Sketch带来了空间和时间方面明显的性能提升，已被广泛地应用到许多应用场景。



## Simply Sampling

**简单抽样：**当面临大量数据需要处理的情况，一个比较简单的想法就是忽略大部分数据，仅从完整的数据中抽取部分数据来进行处理。并将从部分数据集中得到的结论推广至完整数据集。为了使结论更加可信，抽样的数据必须是随机的。



**最基本的抽样方法是：**均匀随机抽样法（uniform random sampling）。对于一个大数据集，我们随机从该大数据集中挑选少量的数据组成Sample，然后，很多问题均可通过简单分析Sample来解决。



**基本问题：**

1. Sample应该抽样多少是数据才能提供一个较好的效果？
	- 大小为s的Sample样本的标准误差与$1/\sqrt{s}$成正比
	- 也即，假设Sample有1000个样本，则误报率大概为3%。如果希望误报率为0.3%，则Sample需要100000个样本。
2. Sample如何抽样，才能保证是均匀随机的？
	- 需要保证每个数据被抽样的概率是相同的
	- 一个简单的方案：给每个数据赋予一个随机数，按照随机数进行排序，取前s个数据作为Sample
3. 当新的数据不断到来，如何维持（更新）Sample？
	- 一个简单的方案：当一个新item数据到来，有p概率将其放入Sample
	- 如果想维持Sample的大小不变，则给所有的数据赋予一个随机标签，然后按照标签排序，永远取前s个标签对应的数据



**应用场景：**

1. 数据库管理系统DBMS中，做查询优化，为了评估不同查询方案的性能，往往会将不同查询方案在Sample进行测试和比较。
2. 在做数据集成时，判断两个Table中两个Columns是否相关时，如果是两个Table的所有数据进行对比，太耗时，因此往往抽样部分数据来提高处理效率。





## Bloom Filter

**布隆过滤器：**是一种近似集合成员资格查询数据结构，用于判断某个元素是否被存储，非常节省空间，但存在误报的情况（假阳性，即元素不存在，但Bloom Filter表示存在）。

布隆过滤器表示元素不存在，则该元素一定不存在；布隆过滤表示元素存在，则该元素可能存在也可能不存在。

（关于布隆过滤器的详细介绍很多，本文就不赘述了）



**应用场景：**

利用布隆过滤器，能够过滤掉大量不符合条件的请求，少量请求需要二次确认。因此，系统性能得到提升。

- 黑名单系统。假设有100亿个黑名单URL，每个URL大小为64字节。则数据库存储这些数据需要640GB，而使用Bloom Filter，如果不允许误报，即为每个URL分配1bit的位置，则需要100亿个1bit，即1.2GB。这是个比较极端的例子，一般来说不会有这么大的数据。
	- 假设有MB级别的黑名单，则一般浏览器不会将其存在用户本地，而是放在服务器上，如果每次访问网页都去请求服务器，不仅耗时，而且服务器承受压力太大。
	- 如果换用Bloom Filter，仅用KB级别就可以存储所有数据，这样就可以放在用户本地。访问网页时，先在本地判断，若Bloom Filter返回不存在，则该网页一定不在黑名单中，若Bloom Filter返回存在，则再去服务器二次确认。
- 分布式数据库（Distributed Database）。如Google’s Bigtable，Apache’s Cassandra，HBase。Bloom Filter作为分布式数据块的索引，记录数据库的什么行或列被存储在磁盘上，以此避免了对磁盘上不存在数据的耗时访问。



## Count-Min Sketch

**背景：**当你需要记录某个元素item出现的次数时，可以考虑用计数器记录，该元素每出现一次，计数器就加一。当你需要记录多个元素出现的次数时，一种简单的方案是，为每个元素设置一个计数器。但是这种方案有两个缺点：（1）每当出现一个新元素时，都需要为其创建计数器（2）当元素出现次数太多时，则计数器的大小就需要设置的很大，比较浪费空间。当计数器的大小超过内存时，则就需要访问慢速的存储器，效率会大大降低。

举个例子，在Twitter中，需要记录每个推文被访问的次数，因此为每个推文设置一个计数器，然而，总推文数量高达几十亿，因此这将耗费大量的空间，此方案不可取。



**Count-Min Sketch：**

它能够保证对于大量的计数需求来说，是准确的；但对于小量的计数需求来说，不是很准确



数据结构：

- 一个二维数组（m×n），数组中的每个位置是一个计数器
- m个哈希函数，哈希函数的映射范围为[0, n]



插入操作：

- 每个哈希函数对应二维数组中的一行，将元素映射到该行上的某个计数器中，计数器加一
- 每行都找一个计数器加一



查询操作：

- 为了查询某个元素出现的次数
- 在数组的每行中找到该元素对应的计数器，把这些计数器中的最小值作为该元素出现的次数



![](.\img\Snipaste_2022-05-26_19-35-07.png)



由于哈希冲突的原因，因此每个计数器不可能被一个元素单独占用，所以我们找数值最小的计数器来近似表示元素出现的次数。





**讨论：**

- 很大概率上，Count-Min Sketch返回的结果是接近真实结果的
- 返回结果的准确性依赖于二维数组的行数和列数



## Log Log Counting

**背景：**基数估计问题（Cardinality Estimation），统计一个集合中不重复元素的个数。一个简单的思路是，准备一个集合S，每当一个元素x到来，先判断x是否已经存储在集合S中，若不存在，则将x加入集合S中，否则不加入。

上述方案在数据量较小时可以使用，但当面对海量数据时，则会面临两个问题：

- 空间方面：需要存储每个曾经出现过的元素，空间消耗直线上升
- 时间方面：每当一个元素到来，均需要在集合中查找该元素是否被存储过。当数据量较大时，查询过程十分耗时

**基本的算法有三个：**

- Linear Counting (LC)
- Log Log Counting (LLC)
- Hyper Log Log Counting (HLLC)



**1. Log Log Counting：**

对于每一个元素，我们对其进行哈希计算得到一串二进制串，记录该二进制串中从左向右数第一个‘1’出现的位置$\rho$。统计所有$\rho$中的最大值为$\rho_{max}$，那么可以粗略估计出集合中元素的数量$n=2^{\rho_{max}}$。

例如：假设集合中有$\{x_1,x_2,x_3,x_4\}$四个元素，分别对应的哈希结果为{0111，0001，0010，0010}，那么$\rho_{max}=2$，则估计$n=2^2=4$

**解释：**

- 统计第一个‘1’出现的位置是概率论中的伯努利过程，换个简单的说法，连续掷硬币，直到出现正面朝上的情况停止，统计掷硬币的次数。与从二进制串中找第一个‘1’出现的位置是同样的道理。
- 投掷1次硬币得到正面的概率为$1/2$，投掷2次硬币得到正面的概率为$1/2^2$，……，投掷k次硬币得到正面的概率为$1/2^k$。

- 问题一：做n次上述游戏，则每次游戏中投掷次数均不大于k的概率$P_1$？
	- 一次游戏中投掷k次硬币得到正面的概率为$1/2^k$，则一次游戏中投掷次数大于k才能得到正面的概率为$1-1/2^k$，则n次游戏中投掷次数大于k才能得到正面的概率为$(1-1/2^k)^n$
- 问题二：做n次上述游戏，则至少有一次游戏中投掷次数均等于k的概率$P_2$？
	- 至少一次游戏中投掷次数等于k的概率=1 - 做n次游戏投掷次数均不为k的概率 = $1-{(1-1/2^{k-1})}^n$
	- 做n次游戏投掷次数均不为k的概率 = （做1次游戏投掷次数不为k的概率）^n = ${(1-1/2^{k-1})}^n$
	- 做1次游戏投掷次数不为k的概率 = 1 - 投掷k次硬币得到正面的概率 = $1-1/2^{k-1}$
	- 投掷k次硬币得到正面的概率 = 连续k-1次投掷都是反面 = $1/2^{k-1}$

- 当$n \gg 2^k$，$P_1$的概率几乎为0；当$n \ll 2^k$，$P_2$的概率几乎为0；



用自然语言概括上述结论就是：当游戏次数远远小于时$2^k$，至少有一次过程投掷次数等于k的概率几乎为0；当伯努利过程次数远远大于$2^k$时，没有一次过程投掷次数大于k的概率也几乎为0。

**设一个集合的基数为n，$\rho_{max}$为所有元素中首个“1”的位置最大的那个元素的“1”的位置，如果n远远小于$2^{\rho_{max}}$，则我们得到$2^{\rho_{max}}$为当前值的概率几乎为0（它应该更小），同样的，如果n远远大于$2^{\rho_{max}}$，则我们得到$2^{\rho_{max}}$为当前值的概率也几乎为0（它应该更大），因此$2^{\rho_{max}}$可以作为基数n的一个粗糙估计。**



**分桶平均：**

使用上述单一估计量（‘1’出现的最大位置）进行估计可能误差较大。因此，采用分桶平均的方式来减少误差。

具体来说，将一个长为L的二进制比特串分为两部分，前m位作为桶索引，后L-m位作为前面用于计算‘1’位置的比特串。由于进行了分桶，因此我们统计每个桶的$\rho_{max}$（这里记为M[i]）然后求平均数。

最终基数估计为$n=2^{\sum_{i=0}^{2^m}{M[i]}/2^m}$



**2. Hyper Log Log Counting：**

在分桶平均的基础上进行的优化。

由于算术平均数对于离群值特别敏感，因此当n的数量较少时，LLC的基数估计结果不是很准确。

HLLC用[调和平均数](https://www.zhihu.com/question/23096098)代替了算术平均数。

$\rho_{max}=\dfrac{2^m}{\dfrac{1}{M[0]}+\dfrac{1}{M[1]}+...+\dfrac{1}{M[2^m]}}=\dfrac{2^m}{\sum_{i=0}^{2^m}\dfrac{1}{M[i]}}$





> **参考：**
>
> Data Sketching：https://dl.acm.org/doi/10.1145/3080008
>
> Log Log Counting：https://www.yuque.com/techcats/algo/an1umb
>
> 调和平均数：https://www.zhihu.com/question/23096098